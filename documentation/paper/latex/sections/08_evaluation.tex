\section{Evaluation}

We validate NormCode through two complementary demonstrations: a \textbf{self-hosted compiler pipeline} (demonstrating expressive completeness) and a \textbf{base-X addition algorithm} (demonstrating correctness and debuggability).

\subsection{Case Study 1: The Self-Hosted Derivation Pipeline}

The most significant validation of NormCode is that \textbf{its own compiler pipeline is implemented as a NormCode plan}.

The task was to transform a high-level natural language instruction into an executable NormCode plan. The plan used a five-phase, \textasciitilde50-inference NormCode structure. The orchestrator successfully executed this plan, producing the JSON repositories and runner scripts.

This self-hosting demonstrates: (1) \textbf{Expressive Completeness} (handling loops, conditionals, human-in-the-loop); (2) \textbf{Practical Viability} of the orchestrator/checkpointing system; and (3) \textbf{Recursive Validation} (testing NormCode by running its own pipeline).

\subsection{Case Study 2: Base-X Addition (Correctness Validation)}

To validate correctness, we implemented a base-X addition algorithm.

The task was adding two arbitrary-base numbers digit-by-digit. The NormCode plan decomposed into \textasciitilde25 inferences organized around three nested loops (outer loop over number pairs, inner loops for unit-place extraction and appending). The Concept Repository covered 50+ ground and intermediate concepts. Key mechanisms demonstrated include \textbf{Loop Quantification}, \textbf{Conditional Branching}, \textbf{Grouping}, and \textbf{Timing Dependencies}.

\textbf{The Result}: The orchestrator achieves **100\% accuracy** on the addition task across test suites of varying digit lengths.

Because NormCode ultimately generates code (via paradigms), the final output is deterministic. NormCode structures the \textit{derivation} of that code. Full repository files are in \textbf{Appendix A}.

\subsection{Quantitative Observations}

Formal experiments are pending, but qualitative observations include:

\begin{table}[t!]
\centering
\begin{tabular}{lp{0.6\linewidth}}
\toprule
\textbf{Dimension} & \textbf{Observation} \\ 
\midrule
\textbf{Debuggability} & Flow indices enable precise localization of failures (e.g., ``Step 1.3.2 failed because input X was empty''). \\ 
\textbf{Token Efficiency} & Perceptual signs reduce token cost by passing pointers instead of full data. Syntactic operations are free. \\ 
\textbf{Resumability} & Checkpointing allows runs to be paused and resumed, or forked for experimentation. \\ 
\textbf{Auditability} & Every inference's inputs and outputs are logged, enabling post-hoc analysis of AI decisions. \\ 
\bottomrule
\end{tabular}
\caption{Qualitative observations from case studies.}
\label{tab:evaluation_observations}
\end{table}

Comparative evaluation against direct prompting and other frameworks (LangChain, etc.) is planned for future work.

