### Step 1.1: Automated Instruction Distillation

The process begins by feeding the raw user prompt to an LLM guided by a specialized meta-prompt. This meta-prompt instructs the model to perform a sophisticated analysis, separating the core procedural instructions from all other contextual information.

The goal is to produce one key artifact:

1.  **Instruction Block:** This contains the clean, unambiguous, and procedural logicâ€”the "what to do." It is synthesized from the core request in the user's prompt. This is a markdown file.

---
*From raw--prompt.md:*

#### **Example: Simple & Casual Request**

-   **Input (Raw Prompt):** `"hey can you just get me the latest sales report and email it to me and Sarah? sarah@example.com"`
-   **Output (Instruction Block):** `"Generate the latest sales report. Then, send an email with the report attached to analyst@example.com and sarah@example.com."`

#### **Example: Complex & Technical Request**

-   **Input (Raw Prompt):**
    ```markdown
    Okay, here's the workflow for deploying the new feature.
    1.  Run the migration script on the staging database.
    2.  If it succeeds, deploy the `feature-branch` to the staging environment.
    3.  Notify the #dev-ops channel on Slack with the results.
    
    Make sure the deployment only happens outside of peak hours (9am-5pm PST).
    ```
-   **Output (Instruction Block):**
    ```markdown
    Run the migration script on the staging database. If the migration is successful, deploy the feature-branch to the staging environment and send a Slack message to the #dev-ops channel with the results.
    ```
