# Step 4.2: Automated Prompt Generation

**Objective:** To assemble the final, executable prompt files by combining a base prompt template with the specific inputs and context required for each prompt-driven inference. This is a deterministic assembly step, not an LLM generation step.

**Process:**

This step is an automated script that takes two key inputs:

1.  **The final Context Manifest (`4.1_context_manifest.json`):** This file provides the complete mapping between each prompt and its required context files.
2.  **The formalized NormCode plan (`3.3_formalized.nc`):** This file defines the explicit input-output relationships for each inference.

The script performs the following actions:

1.  **Reads the Manifest and Plan:** It parses both the `4.1_context_manifest.json` and `3.3_formalized.nc` files.
2.  **Iterates Through Prompts:** It loops through each prompt defined in the manifest.
3.  **Identifies Base Prompt and Inputs:** For each prompt, it identifies the base prompt template (e.g., `instruction_distillation_prompt.md`). It then analyzes the corresponding inference in the `.nc` file to identify all its inputs. It must recognize grouped inputs (defined with `&in`) as a dictionary or a list of files that correspond to a single input placeholder.
4.  **Assembles Content:** The script reads the base prompt template. It then systematically replaces the input placeholders (e.g., `$input_1`, `$input_2`) with the content of the corresponding input files defined in the `.nc` plan. Any additional context files from the manifest that are not explicit inputs are prepended to the final prompt.
5.  **Writes Prompt Files:** The fully assembled content is written to a new file within the `prompts/` directory. The name of the file is taken directly from the key in the manifest (e.g., `[1.2.2.2.]1.1_instruction_distillation.md`).

---

### Example Walkthrough

This example demonstrates how a single prompt file is assembled using a base template and structured inputs.

**1. Inputs:**

*   **`4.1_context_manifest.json` snippet:**
    ```json
    {
      "context_mapping": {
        "[1.2.2.2.]1.1_instruction_distillation.md": {
          "used_by_inference": "1.2.2.1.imperative|<= ::{%(direct)}({prompt}<$({instruction distillation prompt})%>: {1}<$({input files})%>)",
          "context_files": [
            "./context_store/1.2.2.1---automated_instruction_distillation.md",
            "./context_store/shared---pipeline_goal_and_structure.md",
            "./context_store/shared---file_info-instruction_block.md"
          ]
        }
      }
    }
    ```
*   **`3.3_formalized.nc` snippet:**
    ```normcode
    1.2.2.1.imperative|<= ::{%(direct)}({prompt}<$({instruction distillation prompt})%>: {1}<$({input files})%>)
    1.2.2.3.object|<- {input files}<:{1}>
        1.2.2.3.1.grouping|<= &in
        1.2.2.3.2.object...|<- {original prompt}
        1.2.2.3.3.object|<- {other input files}
    ```
*   **Content of Context Files:**
    *   `context_store/1.2.2.1---automated_instruction_distillation.md` (This file acts as the prompt template):
        ```markdown
        Distill the core instruction from the following inputs:
        ---
        $input_1
        ```

**2. Assembly Process:**

The script generates a structured prompt by performing the following steps for each entry in the manifest:

1.  **Identifies Context Files:** It retrieves the list of `context_files` for the prompt.
2.  **Constructs Structured Markdown:** Instead of simple concatenation, the script builds a markdown document with a clear hierarchy:
    *   It creates a main `# [context]` section. Under this, it iterates through the shared context files (e.g., `shared---*.md`), creating a sub-header from each filename (e.g., `## [pipeline_goal_and_structure]`) and inserting the file's content.
    *   It then creates a `# [TASK]` section. This section is populated with content from the remaining, more specific context files. The script identifies the main instruction file (e.g., `1.2.2.1---...md`) and places its content under `## [MAIN INSTRUCTION]`.
    *   Finally, it appends the predefined `## [INPUT]` and `## [OUTPUT FORMAT]` sections, which contain the necessary placeholders and formatting instructions for the orchestrator.
3.  **Writes the Final Prompt:** The complete, structured markdown string is written to the corresponding file in the `prompts/` directory.

**3. Output (Final content of the generated prompt file):**

The result is a complete, well-structured prompt file where the template, context, and inputs are clearly delineated.

```markdown
# [context]

## [pipeline_goal_and_structure]
<Content of shared---pipeline_goal_and_structure.md>

## [file_info-instruction_block]
<Content of shared---file_info-instruction_block.md>

---
# [TASK]

## [MAIN INSTRUCTION]
<Content of 1.2.2.1---automated_instruction_distillation.md>


## [INPUT]

### Input:

**A dictionary of original prompt and other input files:**
$input_1

## [OUTPUT FORMAT]
The output should be about instruction block. Execute the instruction and return a JSON object with "analysis" (your reasoning) and "answer" (a dictionary with "instruction block"). Return only the JSON object.
```

This process is repeated for every entry in the manifest, resulting in a fully populated `prompts/` directory where each prompt is correctly assembled with its specific inputs and context.
